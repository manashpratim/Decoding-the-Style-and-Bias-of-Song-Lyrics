{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pickle.load( open( \"bb.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(text):\n",
    "    #tokens = word_tokenize(text.lower())\n",
    "    #lmtzr = WordNetLemmatizer()\n",
    "    #p=[]\n",
    "    #for token, tag in pos_tag(tokens):\n",
    "        #lemma = str(lmtzr.lemmatize(token, tag_map[tag[0]]))\n",
    "        #p.append(lemma)\n",
    "        #words = [w for w in p if not w in stop_words]  \n",
    "    #return ' '.join(words)\n",
    "    return ' '.join([w for w in word_tokenize(text.lower()) if not w in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWords(A):\n",
    "        dic={}\n",
    "        for x in A:\n",
    "            if not x in  dic:        \n",
    "                  dic[x] = A.count(x)\n",
    "        sorted_items=[pair[0] for pair in sorted(dic.items(), key=lambda item: item[1])]\n",
    "        return sorted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_rank(items):\n",
    "    z={}\n",
    "    for i in range(len(items)):\n",
    "        z[items[i]]= len(items)-i\n",
    "    return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981\n",
      "1967\n",
      "1989\n",
      "1976\n",
      "1990\n",
      "1975\n",
      "2002\n",
      "2000\n",
      "1977\n",
      "1970\n",
      "1985\n",
      "2007\n",
      "2009\n",
      "1997\n",
      "2014\n",
      "2004\n",
      "1999\n",
      "1992\n",
      "1995\n",
      "2005\n",
      "2006\n",
      "2003\n",
      "1996\n",
      "1993\n",
      "2011\n",
      "2010\n",
      "1998\n",
      "1991\n",
      "1972\n",
      "1980\n",
      "1979\n",
      "1994\n",
      "2013\n",
      "1986\n",
      "2012\n",
      "1983\n",
      "1969\n",
      "2001\n",
      "1968\n",
      "2015\n",
      "1966\n",
      "1988\n",
      "1982\n",
      "1965\n",
      "1984\n",
      "2008\n",
      "1971\n",
      "1978\n",
      "1973\n",
      "1974\n",
      "1987\n"
     ]
    }
   ],
   "source": [
    "dic={}\n",
    "for year in data:\n",
    "        print(year)\n",
    "        dic[year]=word_rank(countWords(stopwords( ''.join(data[year])).split()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('rank.pickle', 'wb') as handle:\n",
    "    pickle.dump(dic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=['rock','blues']\n",
    "w={}\n",
    "for i in range(len(l)):\n",
    "    for year in dic:\n",
    "            if year not in w:\n",
    "                w[year]=[]\n",
    "                if l[i] in dic[year]:\n",
    "                    w[year].append(dic[year][l[i]])\n",
    "                else:\n",
    "                    w[year].append(len(dic[year]))\n",
    "            else:\n",
    "                if l[i] in dic[year]:\n",
    "                    w[year].append(dic[year][l[i]])\n",
    "                else:\n",
    "                    w[year].append(len(dic[year]))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_rank.csv', 'a',errors='replace') as the_file:\n",
    "    l=['rock','blues']\n",
    "    #the_file.write('Year'+','+str(l[0])+','+str(l[1])+','+str(l[2])+','+str(l[3])+'\\n')\n",
    "    the_file.write('Year'+','+str(l[0])+','+str(l[1])+'\\n')\n",
    "    for year in w:\n",
    "        the_file.write(str(year)+','+str(w[year][0])+','+str(w[year][1])+'\\n')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
